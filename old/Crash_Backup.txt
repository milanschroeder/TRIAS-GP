#########################################################################
# Project:  TRIAS - Geopolitics
# Tasks:    Extract semantic similarity token weigths from GloVe vectors
#           for scaling Commission Communication later on
# Author:   @ChRauh (21.02.2025)
#########################################################################

# Inputs: tokens.rds; glove.6B.300d.rds
# Output: semantic similarity weights (token-level) for various concept and respective visualizations


# Packages ####
library(tidyverse) # Easily Install and Load the 'Tidyverse' CRAN v2.0.0 
library(text2vec) # Modern Text Mining Framework for R CRAN v0.6.4 
library(newsmap) # Semi-Supervised Model for Geographical Document Classification CRAN v0.9.0 
library(ggwordcloud) # A Word Cloud Geom for 'ggplot2' CRAN v0.6.1 
library(quanteda) # Quantitative Analysis of Textual Data CRAN v4.0.2
library(GGally) # Extension to 'ggplot2' CRAN v2.2.1

# fix randomness
set.seed(1905)

# Paths ####
# Needed as big files currently not part of the repo

# data_path <- "~/Nextcloud/Shared/TRIAS Brückenprojekt/Daten/" # MS/WZB
# data_path <- "C:/Users/rauh/NextCloudSync/Shared/Idee Brückenprojekt Ju-Chri/Daten/" # CR/WZB
data_path <- "D:/WZB-Nextcloud/Shared/Idee Brückenprojekt Ju-Chri/Daten/" # CR/HP


# Key geographical entities to remove before scaling ####

# As we want to scale the context in which countries are mentioned in Comm communication
# geography itself should not affect scaling weights

geo_entities <- c(c(newsmap::data_dictionary_newsmap_en %>% unlist(),
                    newsmap::data_dictionary_newsmap_ar %>% unlist(),
                    newsmap::data_dictionary_newsmap_de %>% unlist(),
                    newsmap::data_dictionary_newsmap_es %>% unlist(),
                    newsmap::data_dictionary_newsmap_fr %>% unlist(),
                    newsmap::data_dictionary_newsmap_he %>% unlist(),
                    newsmap::data_dictionary_newsmap_it %>% unlist(),
                    newsmap::data_dictionary_newsmap_ja %>% unlist(),
                    newsmap::data_dictionary_newsmap_pt %>% unlist(),
                    newsmap::data_dictionary_newsmap_pt %>% unlist(),
                    newsmap::data_dictionary_newsmap_ru %>% unlist(),
                    newsmap::data_dictionary_newsmap_zh_cn %>% unlist(),
                    newsmap::data_dictionary_newsmap_zh_tw %>% unlist()
) %>% tibble(entities = .) %>% 
  mutate(entities = ifelse(str_length(entities %>% str_remove("\\*")) < 6 | str_detect(entities, "\\*"), 
                           paste0("\\b", entities %>% str_remove("\\*"), "\\b"),
                           entities %>% str_remove("\\*"))) %>% 
  filter(str_detect(entities, "\\bus\\b", negate = T)) %>%  # remove us bc false positives
  pull(entities) %>% unique(),
"afri", "\\beurop", "americ", "oceania", "asian", "\\busa\\b")



# Pre-trained word-embedding model ####

# Using the ready-made GLOVE word embeddings trained on Wikipedia and Gigaword
# Version with 400k vocabulary and 300 dimensions

# Get it here: https://nlp.stanford.edu/projects/glove/ # ! LARGE FILE !

# I have parsed this for other purposes already, if you start from the raw file see:
# https://gist.github.com/tjvananne/8b0e7df7dcad414e8e6d5bf3947439a9

glove <- read_rds(paste0(data_path, "external_data/glove.6B.300d.rds"))

# Clean up the vocabulary a bit 
# lots of rare trash in there, exclude stopwords and geo entities
# takes some time ...
vocab <- names(glove) %>% 
  as.data.frame() %>% 
  rename(token = 1) %>% 
  filter(str_detect(token, "[a-z]")) %>% 
  filter(nchar(token) > 1) %>% 
  filter(!(token %in% stopwords("english"))) %>% 
  filter(!str_detect(token, "\\.")) %>% 
  filter(!str_detect(token, "[0-9]")) %>% 
  filter(!str_detect(token, geo_entities %>% paste(collapse = "|"))) # Remove country/continent names - they should not affect the scale
glove <- glove %>% select(vocab$token)
rm(vocab)
gc()

# Store (to save time in future iterations)
# write_rds(glove, paste0(data_path, "external_data/glove.6B.300d_GPfilter.rds"))
# glove <- read_rds(paste0(data_path, "external_data/glove.6B.300d_GPfilter.rds"))



# Function to find nearest neighbors in word vectors model ####

# Taken from: https://gist.github.com/tjvananne/8b0e7df7dcad414e8e6d5bf3947439a9
# N.B.: Expects vectors with tokens in columns and dimensions in rows 
find_sim_wvs <- function(this_wv, all_wvs, top_n_res=40) {
  # this_wv will be a numeric vector; all_wvs will be a data.frame with words as columns and dimesions as rows
  require(text2vec)
  this_wv_mat <- matrix(this_wv, ncol=length(this_wv), nrow=1)
  all_wvs_mat <- as.matrix(all_wvs)
  
  if(dim(this_wv_mat)[[2]] != dim(all_wvs_mat)[[2]]) {
    print("switching dimensions on the all_wvs_matrix")
    all_wvs_mat <- t(all_wvs_mat)
  }
  
  cos_sim = sim2(x=all_wvs_mat, y=this_wv_mat, method="cosine", norm="l2")
  sorted_cos_sim <- sort(cos_sim[,1], decreasing = T) 
  return(head(sorted_cos_sim, top_n_res))
}




# Semantic similarity to ECONOMY terms ####

# Seed tokens
# Identical to the Rauh/EU-actorness paper as there is initial human validation
econ_vocab <- c("trade", "economy", "market", "business", "commerce")

# The average vector of these seeds in the pre.trained word embeddings
econ_vector <- glove %>% 
  select(all_of(econ_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
econ_simils <-
  find_sim_wvs(econ_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(sim.target = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% econ_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(sim.target)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Export token weights
# Export semantic similarity dictionary ####
# write_rds(econ_simils, paste0(data_path, "glove_models/SemSimilWeights-Economy.rds", compress = "gz"))


# Visualize 
pl <-
  ggplot(econ_simils %>% ungroup() %>% arrange(desc(sim.target)) %>% head(250), 
         aes(label = token, size = sim.target, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'economy\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Economy.png", pl, width = 24, height = 12, units = "cm")





# Semantic similarity to SECURITY terms ####

# Seed tokens
# Identical to the Rauh/EU-actorness paper as there is initial human validation
sec_vocab <- c("security", "war", "military", "terrorism", "peace")

# The average vector of these seeds in the pre.trained word embeddings
sec_vector <- glove %>% 
  select(all_of(sec_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
sec_simils <-
  find_sim_wvs(sec_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(sim.target = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% sec_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(sim.target)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Export token weights
# Export semantic similarity dictionary ####
# write_rds(sec_simils, paste0(data_path, "glove_models/SemSimilWeights-Security.rds", compress = "gz"))


# Visualize 
pl <-
  ggplot(sec_simils %>% ungroup() %>% arrange(desc(sim.target)) %>% head(250), 
         aes(label = token, size = sim.target, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'security\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Security.png", pl, width = 24, height = 12, units = "cm")




# Semantic similarity to LIBERAL DEMOCRACY terms ####

# Seed tokens
# Identical to the Rauh/EU-actorness paper as there is initial human validation
lib_vocab <- c("rights", "human", "freedom", "democracy", "law")

# The average vector of these seeds in the pre.trained word embeddings
lib_vector <- glove %>% 
  select(all_of(lib_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
lib_simils <-
  find_sim_wvs(lib_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(sim.target = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% lib_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(sim.target)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Export token weights
# write_rds(lib_simils, paste0(data_path, "glove_models/SemSimilWeights-LibDem.rds", compress = "gz"))


# Visualize 
pl <-
  ggplot(lib_simils %>% ungroup() %>% arrange(desc(sim.target)) %>% head(250), 
         aes(label = token, size = sim.target, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'liberal rights\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_LibDem.png", pl, width = 24, height = 12, units = "cm")




# Semantic scaling COOPERATION/CONFLICT ####

# Seed tokens
coop_vocab <- c("cooperation", "agreement",    "support",    "collaboration", "unity")
conf_vocab <- c("conflict",    "disagreement", "opposition", "confrontation", "hostility")

# The average vectors of these seeds in the pre.trained word embeddings
coop_vector <- glove %>% 
  select(all_of(coop_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean
conf_vector <- glove %>% 
  select(all_of(conf_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
coop_simils <-
  find_sim_wvs(coop_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(coop = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% coop_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(coop)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector
conf_simils <-
  find_sim_wvs(conf_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(conf = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% conf_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(conf)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Scale
sc <- coop_simils %>% 
  select(token, coop) %>% 
  left_join(conf_simils %>% 
              select(token, conf),
            by = "token") %>% 
  mutate(coop_confl = coop-conf) %>% 
  arrange(desc(coop_confl))

# Export token weights
# write_rds(sc, paste0(data_path, "glove_models/SemSimilWeights-CooperationConflict.rds", compress = "gz"))


# Visualize (ind scales)

pl <-
  ggplot(coop_simils %>% ungroup() %>% arrange(desc(coop)) %>% head(250), 
         aes(label = token, size = coop, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'cooperation\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Cooperation.png", pl, width = 24, height = 12, units = "cm")

pl <-
  ggplot(conf_simils %>% ungroup() %>% arrange(desc(conf)) %>% head(250), 
         aes(label = token, size = conf, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'conflict\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Conflict.png", pl, width = 24, height = 12, units = "cm")


# Visualize scaling weights


# Stratified sample
df <- sc %>% 
  mutate(group = cut(coop_confl, 7)) %>% # Cut range of scale into n intervals
  group_by(group) %>% 
  sample_n(size = 28) %>% # n words from each interval
  ungroup()
  


# Selected terms for highlighting

df.anchors <- sc %>% 
  filter(token %in% c(coop_vocab, conf_vocab)) %>% 
  mutate(group = "Anchor terms defined by researcher")

df.learned <- sc %>% 
  filter(token %in% c("aggression", "mistrust", "anger", "tensions", "frustration", "escalation", "backlash", "insistence",
                      "understanding", "partnership", "solidarity", "negotiate", "compromise", "peaceful", "fiendship", "coalition",
                      "coexistence", "unaccaeptable", "stable", "stop", "normal", "responsible")) %>% 
  mutate(group = "Example terms scaled by the algorithm")

df.highlight <- rbind(df.anchors, df.learned)
df.highlight$group <- fct_rev(factor(df.highlight$group, levels = c("Anchor terms defined by researcher", "Example terms scaled by the algorithm")))

df <- df %>% filter(!(token %in% df.highlight$token)) # Avoid duplicates


pl<- 
  ggplot()+
  #geom_text(data = df[sample(nrow(df), 2500, replace = F), ], alpha= .3, color = "grey60", aes(y=sample.int(100, size = 2500, replace =T), x = conf_coop, label = token)) +
  geom_text(data = df, alpha= .4, color = "grey60", aes(y=sample.int(100, size = nrow(df), replace =T), x = coop_confl, label = token)) +
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_text(data = df.highlight, aes(y=sample.int(100, size = nrow(df.highlight), replace =T), x = coop_confl, label = token, color = group), fontface = "bold")+
  scale_color_manual(values = c("#0380b5", "#9e3173"), name = "")+
  labs(title = "Scaling conflictual vs cooperative language",
       subtitle = paste0("Based on Glove.6B.300d word vector model and a stratified random sample of ", nrow(df), " words from its vocabulary"),
       y = "",
       x = "Extracted word weights\nbetween conflictual and cooperative language")+
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text = element_text(color = "black"),
        # text = element_text(family = "Dahrendorf"),
        plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(),
        panel.grid = element_blank(),
        axis.text.x = element_text(size = 10),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())

ggsave("./output/glove_plots/CoopConflictScalingWeigths.png", pl, width = 28, height = 14, units = "cm")




# Semantic scaling FRIEND/FOE ####

# Seed tokens
friend_vocab = c("friend", "partner",  "ally",  "peace", "peaceful",   "friendly", "cooperative")
foe_vocab =    c("foe",    "opponent", "enemy", "war",   "aggressive", "hostile",  "uncooperative")

# The average vectors of these seeds in the pre.trained word embeddings
friend_vector <- glove %>% 
  select(all_of(friend_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean
foe_vector <- glove %>% 
  select(all_of(foe_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
friend_simils <-
  find_sim_wvs(friend_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(friend = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% friend_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(friend)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector

foe_simils <-
  find_sim_wvs(foe_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(foe = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% foe_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(foe)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Scale
sc <- friend_simils %>% 
  select(token, friend) %>% 
  left_join(foe_simils %>% 
              select(token, foe),
            by = "token") %>% 
  mutate(friend_foe = friend-foe) %>% 
  arrange(desc(friend_foe))

# Export token weights
# write_rds(sc, paste0(data_path, "glove_models/SemSimilWeights-FriendFoe.rds", compress = "gz"))


# Visualize (ind scales)

pl <-
  ggplot(friend_simils %>% ungroup() %>% arrange(desc(friend)) %>% head(250), 
         aes(label = token, size = friend, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'friend\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Friend.png", pl, width = 24, height = 12, units = "cm")

pl <-
  ggplot(foe_simils %>% ungroup() %>% arrange(desc(foe)) %>% head(250), 
         aes(label = token, size = foe, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'foe\' seed terms",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_Foe.png", pl, width = 24, height = 12, units = "cm")


# Visualize scaling weights


# Stratified sample
df <- sc %>% 
  mutate(group = cut(friend_foe, 5)) %>% # Cut range of scale into n intervals
  group_by(group) %>% 
  sample_n(size = 20) %>% # n words from each interval
  ungroup()



# Selected terms for highlighting

df.anchors <- sc %>% 
  filter(token %in% c(friend_vocab, foe_vocab)) %>% 
  mutate(group = "Anchor terms defined by researcher")

df.learned <- sc %>% 
  # ADAPT?!?
  filter(token %in% c("aggression", "mistrust", "anger", "tensions", "frustration", "escalation", "backlash", "insistence",
                      "understanding", "partnership", "solidarity", "negotiate", "compromise", "peaceful", "fiendship", "coalition",
                      "coexistence", "unaccaeptable", "stable", "stop", "normal", "responsible")) %>% 
  mutate(group = "Example terms scaled by the algorithm")

df.highlight <- rbind(df.anchors, df.learned)
df.highlight$group <- fct_rev(factor(df.highlight$group, levels = c("Anchor terms defined by researcher", "Example terms scaled by the algorithm")))

df <- df %>% filter(!(token %in% df.highlight$token)) # Avoid duplicates


pl<- 
  ggplot()+
  #geom_text(data = df[sample(nrow(df), 2500, replace = F), ], alpha= .3, color = "grey60", aes(y=sample.int(100, size = 2500, replace =T), x = friend_foe, label = token)) +
  geom_text(data = df, alpha= .4, color = "grey60", aes(y=sample.int(100, size = nrow(df), replace =T), x = friend_foe, label = token)) +
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_text(data = df.highlight, aes(y=sample.int(100, size = nrow(df.highlight), replace =T), x = friend_foe, label = token, color = group), fontface = "bold")+
  scale_color_manual(values = c("#0380b5", "#9e3173"), name = "")+
  labs(title = "Scaling friend vs foe",
       subtitle = paste0("Based on Glove.6B.300d word vector model and a stratified random sample of ", nrow(df), " words from its vocabulary"),
       y = "",
       x = "Extracted word weights\nbetween friend and foe")+
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text = element_text(color = "black"),
        # text = element_text(family = "Dahrendorf"),
        plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(),
        panel.grid = element_blank(),
        axis.text.x = element_text(size = 10),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())

ggsave("./output/glove_plots/FriendFoeScalingWeigths.png", pl, width = 28, height = 14, units = "cm")





# Semantic scaling HOSTILITY/GTI ####

# Seed tokens
# As in the Geopolitical Threat Index paper (Trubowitz/Watanabe 20121): https://academic.oup.com/isq/article/65/3/852/6278490
# Dictionary loaded here is from their replication file

dict <- dictionary(file = paste0(data_path, 'external_data/GTI_keywords.yml'))
friendly_vocab = dict$seedwords$friendly
hostile_vocab = dict$seedwords$hostile

# The average vectors of these seeds in the pre.trained word embeddings
friendly_vector <- glove %>% 
  select(all_of(friendly_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean
hostile_vector <- glove %>% 
  select(all_of(hostile_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
  rowMeans() # Aggregation by mean

# Extract cosine similarities to this average vector
# (word weights that would semantically pull a text towards the seed)
friendly_simils <-
  find_sim_wvs(friendly_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(friendly = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% friendly_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(friendly)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector

hostile_simils <-
  find_sim_wvs(hostile_vector, glove, top_n_res = 400000) %>% 
  as.data.frame() %>% 
  rename(hostile = 1) %>% 
  rownames_to_column("token") %>% 
  mutate(seed = token %in% hostile_vocab) %>% 
  filter(!(token %in% quanteda::stopwords("english"))) %>% 
  arrange(desc(hostile)) %>% 
  mutate(rank.simil=row_number()) %>% 
  relocate(rank.simil) # rank by similarity to seed vector


# Scale
sc <- friendly_simils %>% 
  select(token, friendly) %>% 
  left_join(hostile_simils %>% 
              select(token, hostile),
            by = "token") %>% 
  mutate(gti = friendly-hostile) %>% 
  arrange(desc(gti))

# Export token weights
# write_rds(sc, paste0(data_path, "glove_models/SemSimilWeights-GTI.rds", compress = "gz"))


# Visualize (ind scales)

pl <-
  ggplot(friendly_simils %>% ungroup() %>% arrange(desc(friendly)) %>% head(250), 
         aes(label = token, size = friendly, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'friendly\' seed terms (GTI)",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_GTI_friendly.png", pl, width = 24, height = 12, units = "cm")

pl <-
  ggplot(hostile_simils %>% ungroup() %>% arrange(desc(hostile)) %>% head(250), 
         aes(label = token, size = hostile, color = seed)) +
  geom_text_wordcloud() +
  scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
  scale_colour_manual(values = c("blue", "red"))+
  labs(title = "Semantic similarity to vector of \'hostile\' seed terms (GTI)",
       subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
  theme_minimal()+
  theme(plot.background = element_rect(color = "white"))
ggsave("./output/glove_plots/SemanticallySimilarTerms_GTI_hostile.png", pl, width = 24, height = 12, units = "cm")


# Visualize scaling weights


# Stratified sample
df <- sc %>% 
  mutate(group = cut(gti, 5)) %>% # Cut range of scale into n intervals
  group_by(group) %>% 
  sample_n(size = 25) %>% # n words from each interval
  ungroup()



# Selected terms for highlighting

df.anchors <- sc %>% 
  filter(token %in% c(friendly_vocab, hostile_vocab)) %>% 
  mutate(group = "Anchor terms defined by Trubowitz/Watanabe (2021)")

# Highlighting the same terms as in the GTI paper (interesting whther they occur in GloVe, though)
df.learned <- sc %>% 
  filter(token %in% dict$highlight) %>% 
  mutate(group = "Example terms scaled by the algorithm")

df.highlight <- rbind(df.anchors, df.learned)
df.highlight$group <- fct_rev(factor(df.highlight$group, levels = c("Anchor terms defined by Trubowitz/Watanabe (2021)", "Example terms scaled by the algorithm")))

df <- df %>% filter(!(token %in% df.highlight$token)) # Avoid duplicates


pl<- 
  ggplot()+
  #geom_text(data = df[sample(nrow(df), 2500, replace = F), ], alpha= .3, color = "grey60", aes(y=sample.int(100, size = 2500, replace =T), x = friendly_hostile, label = token)) +
  geom_text(data = df, alpha= .4, color = "grey60", aes(y=sample.int(100, size = nrow(df), replace =T), x = gti, label = token)) +
  geom_vline(xintercept = 0, linetype = "dashed")+
  geom_text(data = df.highlight, aes(y=sample.int(100, size = nrow(df.highlight), replace =T), x = gti, label = token, color = group), fontface = "bold")+
  scale_color_manual(values = c("#0380b5", "#9e3173"), name = "")+
  labs(title = "Scaling friendly vs hostile",
       subtitle = paste0("Based on Glove.6B.300d word vector model and a stratified random sample of ", nrow(df), " words from its vocabulary"),
       y = "",
       x = "Extracted word weights\nbetween friendly and hostile")+
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text = element_text(color = "black"),
        # text = element_text(family = "Dahrendorf"),
        plot.title = element_text(face = "bold", size = 14),
        plot.subtitle = element_text(),
        panel.grid = element_blank(),
        axis.text.x = element_text(size = 10),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank())

ggsave("./output/glove_plots/GTI_ScalingWeigths.png", pl, width = 28, height = 14, units = "cm")




# # Semantic similarity to DIGITAL terms - radically simple dictionary ####
# # See respective markdown in TRIAS-Development for expanded reasoning
# 
# # Seed tokens
# dp_vocab <- c("digital", "online", "computer", "internet", "algorithm")
# 
# # The average vector of these seeds in the pre.trained word embeddings
# dp_vector <- glove %>% 
#   select(all_of(dp_vocab)) %>% # Probably needs a check whether all seeds exist in word vector
#   rowMeans() # Aggregation by mean
# 
# # Extract cosine similarities to this average vector
# # (word weights that would semantically pull a text towards the seed)
# dp_simils <-
#   find_sim_wvs(dp_vector, glove, top_n_res = 400000) %>% 
#   as.data.frame() %>% 
#   rename(sim.target = 1) %>% 
#   rownames_to_column("token") %>% 
#   mutate(seed = token %in% dp_vocab) %>% 
#   filter(!(token %in% quanteda::stopwords("english"))) %>% 
#   arrange(desc(sim.target)) %>% 
#   mutate(rank.simil=row_number()) %>% 
#   relocate(rank.simil) # rank by similarity to seed vector
# 
# 
# # Export token weights
# # Export semantic similarity dictionary ####
# # write_rds(dp_simils, paste0(data_path, "glove_models/SemSimilWeights-DigitalitySimple.rds", compress = "gz"))
# 
# 
# # Visualize 
# pl <-
#   ggplot(dp_simils %>% ungroup() %>% arrange(desc(sim.target)) %>% head(250), 
#        aes(label = token, size = sim.target, color = seed)) +
#   geom_text_wordcloud() +
#   scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
#   scale_colour_manual(values = c("blue", "red"))+
#   labs(title = "Semantic similarity to vector of \'digital\' seed terms",
#        subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
#   theme_minimal()+
#   theme(plot.background = element_rect(color = "white"))
# ggsave("./output/glove_plots/SemanticallySimilarTerms_Digital_Simple.png", pl, width = 24, height = 12, units = "cm")



# # Semantic similarity to DIGITAL terms - intersubjective dictionary ####
# 
# # Seed tokens
# 
# # Five human coders received a list of the top-1500 terms semantically close to the radically simplified dictionary
# # Each coder rated each terms as "Clearly and exclusively referring to the digital technology?" [0|1]
# 
# dp_vocab2 = read_rds("../TRIAS-paper1/large_data/SemSimilWeights-DigitalityAdvancedFreqCorrection.rds") %>% filter(seed == T) %>% pull(token)
# 
# # ... and get back their average vector in the word embedding model  
# dp_vector2 <- glove %>% 
#   select(all_of(dp_vocab2)) %>% # Probably needs a check whether all seeds exist in word vector
#   rowMeans() # Aggregation by mean
# 
# # Extract cosine similarities to this average vector
# # (word weights that would semantically pull a text towards the seed)
# dp_simils2 <-
#   find_sim_wvs(dp_vector2, glove, top_n_res = 400000) %>% 
#   as.data.frame() %>% 
#   rename(sim.target = 1) %>% 
#   rownames_to_column("token") %>% 
#   mutate(seed = token %in% dp_vocab2) %>% 
#   filter(!(token %in% quanteda::stopwords("english"))) %>% 
#   arrange(desc(sim.target)) %>% 
#   mutate(rank.simil=row_number()) %>% 
#   relocate(rank.simil) # rank by similarity to seed vector
# 
# 
# # Export token weights
# # Export semantic similarity dictionary ####
# # write_rds(dp_simils2, paste0(data_path, "glove_models/SemSimilWeights-DigitalityAdvanced.rds", compress = "gz"))
# 
# # Visualize 
# pl <-
#   ggplot(dp_simils2 %>% ungroup() %>% arrange(desc(sim.target)) %>% head(250), 
#          aes(label = token, size = sim.target, color = seed)) +
#   geom_text_wordcloud() +
#   scale_radius(range = c(1.5, 6), limits = c(0, NA)) +
#   scale_colour_manual(values = c("blue", "red"))+
#   labs(title = "Semantic similarity to vector of \'digital\' seed terms",
#        subtitle = "Based on Glove.6B.300d model; Top-250 words most similar to average word vector of the seed terms (based on human coders)\nWords in red are seed terms, words in blue are \'learned\' from the pre-trained word vector model.")+
#   theme_minimal()+
#   theme(plot.background = element_rect(color = "white"))
# ggsave("./output/glove_plots/SemanticallySimilarTerms_Digital_Advanced.png", pl, width = 24, height = 12, units = "cm")



# Compare the scales (token-level) ####


comp.df <- 
  read_rds(paste0(data_path, "glove_models/SemSimilWeights-GTI.rdsgz")) %>% select(token, gti) %>% 
  left_join(read_rds(paste0(data_path, "glove_models/SemSimilWeights-CooperationConflict.rdsgz")) 
            %>% select(token, coop_confl), 
            by = "token") %>% 
  left_join(read_rds(paste0(data_path, "glove_models/SemSimilWeights-FriendFoe.rdsgz")) 
            %>% select(token, friend_foe), 
            by = "token")


comp.pl <- 
  ggpairs(comp.df %>% select(gti, coop_confl, friend_foe) %>% sample_n(50000),
        lower = list(continuous = wrap("points", alpha = 0.05, shape = 16)),  # Set alpha and shape
        upper = list(continuous = wrap("cor", size = 6, color = "black")),
        columnLabels = c("GTI (Hostility)", "Coop-Conflict", "Friend-Foe"))+
  labs(title = "Comparing differently seeded scales on token level",
       subtitle = "GloVe vocabulary, random sample of 50,000 words")+
  theme_bw()+
  theme(
    strip.text = element_text(face = "bold", size = 10)  # Bold and increase facet label size
  )

ggsave("./output/glove_plots/ComparingScalingWeigthsTokenLevel.png", comp.pl, width = 20, height = 20, units = "cm")




# Internal consistency of the three friend/foe scales ####

# My key ideas here - If the respective seed terms form poles of a meaningful scale in the vector space
# defined by the pre-trained model two conditions should be conceptually met 

# A) The seed terms on one side of the scale should be more similar to each other, than to the words on the other side
# -> Can be assed along the cosine similarity of their vectors in the pre-trained space

# B) The lines going from words on one side of the pole to the words on the other end shoudl be parallel (if they capture the same semantic concept)
# -> Can be asseses along the cosine similarity of the difference vectors between all pairs from the opposing side

# Given that the word vector space encodes numerous latent dimensions, these are not to be seen as absolute statements
# But in relative terms, these two metrics should help to asses which scale is more consistent within the meaning space


# Get corresponding for the Friend-Foe scale 

all_ff_words <- unique(c(friend_vocab, foe_vocab)) # Cobine all seed terms from this scale

df.ff <- expand.grid(a = all_ff_words, b = all_ff_words) %>% # All possible combinations of seed terms
  subset(a != b) %>% # Exlcude combinations of a seed term with itself
  mutate(low_a = (a %in% friend_vocab), # Identify the seed pole the words come from
         low_b = (b %in% friend_vocab),
         mixed_pair = (low_a != low_b),
         low_pair = (low_a & low_b),
         high_pair = (!low_a & !low_b)) %>% 
  select(-c(low_a, low_b))
  
# Calculate cosine similarity of each pair in the vector space

df.ff$pair.sime <- NA

for (i in 1:nrow(df.ff)) {
  
  vec_a <- glove %>% select(df.ff$a[i]) %>% pull()
  vec_b <- glove %>% select(df.ff$b[i]) %>% pull()
  
  cos_sim = sim2(x=all_wvs_mat, y=this_wv_mat, method="cosine", norm="l2")
  
}
